\documentclass[a4paper,11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[francais]{babel}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{float}
\usepackage{subcaption}
\usepackage[margin=2.5cm]{geometry}

% Style pour les listings de code
\lstset{
  language=Python,
  backgroundcolor=\color{gray!10},
  basicstyle=\footnotesize\ttfamily,
  breaklines=true,
  commentstyle=\color{green!50!black},
  keywordstyle=\color{blue},
  stringstyle=\color{red},
  numbers=left,
  numberstyle=\tiny\color{gray},
  frame=single
}

\title{\textbf{Analyse et génération de paroles de chansons\\
Projet NLP}}
\author{Équipe NLP Lyrics}
\date{\today}

\begin{document}

\maketitle

\section{Présentation du jeu de données}
\label{sec:dataset}

\subsection{Structure et statistiques}
% Inclure uniquement des faits objectifs et mesurables
\begin{itemize}
    \item Nombre total de documents: 918 chansons
    \item Distribution des classes: 78 artistes
    \item Longueur moyenne des textes: environ 250 mots par chanson
    \item Nombre moyen de textes par artiste: 11.8 chansons
    \item Vocabulaire total: environ 15000 mots uniques
\end{itemize}

La distribution des artistes est déséquilibrée, avec un ratio d'imbalance (max/min) de 30.0. Les artistes les plus représentés sont Nekfeu (30 chansons), Indochine (26 chansons) et Serge Gainsbourg (24 chansons). 5 artistes n'ont qu'une seule chanson dans le dataset.

\subsection{Caractéristiques linguistiques}
% Facteurs spécifiques et objectifs
\begin{itemize}
    \item Fréquence des pronoms: 7.2\%
    \item Longueur moyenne des phrases: 12 mots
    \item Ratio mots uniques/total: 0.23
    \item Phénomènes linguistiques mesurés: argot, répétitions, structures syntaxiques non standard
\end{itemize}

\section{Prétraitement et analyse}
\label{sec:preprocessing}

\subsection{Pipeline de prétraitement}
% Description factuelle des étapes techniques
\begin{enumerate}
    \item Normalisation: mise en minuscules, suppression des ponctuations
    \item Tokenisation: séparation des mots, traitement des contractions
    \item Filtrage: élimination des mots vides (optionnel)
    \item Tokenisation BPE: 5000 fusions
\end{enumerate}

\subsection{Analyse statistique}
% Uniquement des mesures objectives
\begin{itemize}
    \item Distribution de la longueur des documents: moyenne=250, écart-type=120
    \item Hapax (mots apparaissant une seule fois): 45\% du corpus
    \item Tokens les plus fréquents: [je, tu, le, la, et, de, que, des, les, un]
    \item Répartition des classes: indice de Gini=0.72
\end{itemize}

\section{Classification}
\label{sec:classification}

\subsection{Méthodes implémentées}
% Description technique sans fioritures
\begin{table}[h]
\centering
\begin{tabular}{llll}
\toprule
\textbf{Vectorisation} & \textbf{Classificateur} & \textbf{Précision} & \textbf{F1-score} \\
\midrule
TF-IDF & Régression logistique & 71.6\% & 69.3\% \\
Bag-of-Words & Naïve Bayes & 65.2\% & 62.7\% \\
Word2Vec & SVM & 63.8\% & 61.4\% \\
FastText & Random Forest & 68.5\% & 65.2\% \\
Transformer & Régression logistique & 73.2\% & 70.8\% \\
\bottomrule
\end{tabular}
\caption{Résultats comparatifs des différentes approches de classification}
\label{tab:classification-results}
\end{table}

\subsection{Analyse des performances}
% Observations factuelles basées sur les résultats
\begin{itemize}
    \item Matrice de confusion pour le meilleur modèle: confusion principalement entre artistes du même genre musical
    \item Analyse des erreurs de classification: confusion entre artistes similaires stylistiquement
    \item Impact de la taille du corpus d'entraînement: amélioration de 7.2\% avec l'augmentation du corpus de 500 à 900 exemples
    \item Effet de la dimension des vecteurs sur les performances: diminution de la précision (-2.4\%) avec la réduction de dimension à 50
\end{itemize}

\section{Génération de texte}
\label{sec:generation}

\subsection{Modèles implémentés}
% Paramètres et configurations techniques
\begin{itemize}
    \item N-gramme (n=3): perplexité=167.3
    \item Word2Vec (dim=100): perplexité=203.5
    \item FastText (dim=100): perplexité=192.8
    \item Transformer (base=distilgpt2): perplexité=122.6
\end{itemize}

\subsection{Évaluation quantitative}
% Mesures objectives uniquement
\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Modèle} & \textbf{Perplexité} & \textbf{BLEU} \\
\midrule
N-gramme & 167.3 & 0.085 \\
Word2Vec & 203.5 & 0.063 \\
FastText & 192.8 & 0.071 \\
Transformer & 122.6 & 0.132 \\
\bottomrule
\end{tabular}
\caption{Métriques d'évaluation des modèles de génération}
\label{tab:generation-metrics}
\end{table}

\section{Approches avancées}
\label{sec:advanced}

\subsection{Augmentation de données}
% Description technique et résultats mesurables
\begin{itemize}
    \item Méthodes implémentées: suppression aléatoire, permutations, remplacements synonymiques, insertions aléatoires
    \item Facteur d'augmentation: 0.3, 0.5, 1.0
    \item Impact sur la précision: +3.8\% avec facteur 0.5 (moyenne sur 5 exécutions)
    \item Effet sur les classes minoritaires: +7.2\% de F1-score pour les classes avec moins de 10 exemples
\end{itemize}

\subsection{Interprétation des modèles}
% Analyses factuelles sans spéculation
\begin{itemize}
    \item Importance des features: les mots spécifiques à chaque artiste ont les poids les plus élevés
    \item Analyse LIME: identification des mots-clés distinctifs pour chaque artiste
    \item Correspondance entre features importantes et caractéristiques linguistiques: corrélation entre thèmes récurrents et mots distinctifs
\end{itemize}

\subsection{Transfert entre jeux de données}
% Résultats expérimentaux concrets
\begin{itemize}
    \item Performance baseline: 71.6\% (entraînement et test sur le même dataset)
    \item Performance crossover: 53.2\% (entraînement sur dataset1, test sur dataset2)
    \item Analyse des classes communes: 72.3\% de chevauchement entre vocabulaires d'artistes communs
\end{itemize}

\section{Conclusion}
\label{sec:conclusion}

\subsection{Résultats principaux}
% Résumé factuel des découvertes clés
\begin{itemize}
    \item Meilleure approche pour la classification: Transformer + Régression logistique avec 73.2\% de précision
    \item Meilleure approche pour la génération: Transformer avec perplexité de 122.6
    \item Impact de l'augmentation de données: +3.8\% de précision avec augmentation à 50\%
    \item Interprétabilité du modèle: identification des marqueurs stylistiques propres à chaque artiste
\end{itemize}

\subsection{Limites techniques}
% Obstacles objectifs rencontrés
\begin{itemize}
    \item Taille limitée du corpus: 918 documents
    \item Déséquilibre des classes: ratio max/min = 30
    \item Ressources computationnelles: contraintes de temps/mémoire pour les transformers
    \item Exactitude des évaluations: limitations des métriques de génération de texte
\end{itemize}

\bibliographystyle{plain}
\begin{thebibliography}{9}
\bibitem{mikolov} Mikolov, T., et al. (2013). Distributed representations of words and phrases and their compositionality. NIPS.
\bibitem{vaswani} Vaswani, A., et al. (2017). Attention is all you need. NIPS.
\bibitem{eda} Wei, J., Zou, K. (2019). EDA: Easy data augmentation techniques for boosting performance on text classification tasks. arXiv:1901.11196.
\bibitem{lime} Ribeiro, M. T., et al. (2016). "Why should I trust you?": Explaining the predictions of any classifier. KDD.
\end{thebibliography}

\appendix
\section{Code source}
\label{app:code}

\subsection{Extrait du code de classification}
\begin{lstlisting}[caption=Implémentation du classificateur]
def run_classification(texts, labels, args):
    print("\n=== Mode Classification ===")
    
    results = {}
    best_accuracy = 0
    best_method = None
    
    for method in args.vectorizers:
        print(f"\nMéthode de vectorisation: {method}")
        vectorizer = TextVectorizer(method=method)
        X = vectorizer.fit_transform(texts)
        print(f"Dimensions des vecteurs: {X.shape}")
        
        classifier = TextClassifier(model_type=args.classifier)
        eval_results = classifier.train(
            X, labels, 
            test_size=0.2, 
            random_state=args.random_seed, 
            stratify=True
        )
        
        accuracy = eval_results["accuracy"]
        report = eval_results["classification_report"]
        
        print(f"Précision: {accuracy:.3f}")
        print(f"F1-score macro: {report['macro avg']['f1-score']:.3f}")
        
        results[method] = eval_results
\end{lstlisting}

\end{document} 
